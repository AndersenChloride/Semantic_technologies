{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Отчет по предмету \"Анализ распределенных данных\"\n",
        "# студента группы 02121-ДМ Подрядчиков Владимира Валерьевича"
      ],
      "metadata": {
        "id": "39T63SEk61f7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. ETL-процесс и его особенности\n",
        "1. Описание ETL\n",
        "\n",
        "ETL — это аббревиатура, обозначающая три ключевых этапа обработки данных: Extract (извлечение), Transform (преобразование) и Load (загрузка). Эти этапы образуют основу процессов интеграции данных в централизованных системах аналитики и хранилищах данных.\n",
        "\n",
        "Extract (Извлечение)\n",
        "\n",
        "На этом этапе происходит получение данных из различных источников:\n",
        "\n",
        "*   реляционные базы данных (PostgreSQL, MySQL и др.);\n",
        "\n",
        "*    файлы (CSV, Excel, JSON, XML);\n",
        "\n",
        "*    API внешних сервисов;\n",
        "\n",
        "*    облачные хранилища;\n",
        "\n",
        "*    потоковые источники (Kafka, MQTT и др.).\n",
        "\n",
        "Цель этапа — извлечь актуальные и полные данные из разных систем, сохранив их целостность и структуру.\n",
        "\n",
        "Transform (Преобразование)\n",
        "\n",
        "На этапе трансформации данные очищаются, стандартизируются и готовятся к загрузке:\n",
        "\n",
        " *   удаление дубликатов и пропусков;\n",
        "\n",
        " *   преобразование форматов (например, даты, валют);\n",
        "\n",
        " *   нормализация и агрегация;\n",
        "\n",
        " *   объединение таблиц (join);\n",
        "\n",
        " *   расчёт производных показателей (например, суммы, средние значения);\n",
        "\n",
        " *   переименование полей и выравнивание схем.\n",
        "\n",
        "Этот этап наиболее ресурсоёмкий и критичный для качества итоговой аналитики.\n",
        "\n",
        "Load (Загрузка)\n",
        "\n",
        "Завершающий этап — загрузка обработанных данных в целевую систему:\n",
        "\n",
        " *   в хранилище данных ;\n",
        "\n",
        " *   в аналитическую БД;\n",
        "\n",
        " *   в облачные платформы (BigQuery, Snowflake, Redshift и др.);\n",
        "\n",
        " *  в витрины данных или BI-системы.\n",
        "\n",
        "\n",
        "2. Пояснение особенностей ETL\n",
        "\n",
        "Централизованный анализ предполагает, что данные из множества распределённых источников собираются в одно логически или физически централизованное хранилище.\n",
        "\n",
        "Без этапа трансформации данные в разных форматах и с различными структурами будут неконсистентными и бесполезными для комплексной аналитики.\n",
        "\n",
        "Разница между ETL и ELT\n",
        "\n",
        " *   ETL (Extract → Transform → Load): сначала данные обрабатываются, потом загружаются. Подходит для классических хранилищ и маломощных целевых систем.\n",
        "\n",
        " *  ELT (Extract → Load → Transform): сначала всё выгружается в хранилище (например, Data Lake), затем обрабатывается средствами самой системы. Подходит для облачных и масштабируемых платформ.\n",
        "\n",
        "Пример демонстрирует типовой ETL-процесс (Extract, Transform, Load), который часто используется в работе с данными"
      ],
      "metadata": {
        "id": "0P53sWdjC-zH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcjgl52G_sJB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import sqlite3\n",
        "from datetime import datetime\n",
        "\n",
        "df = pd.read_csv('orders.csv')\n",
        "df['order_date'] = pd.to_datetime(df['order_date'], format='%d.%m.%Y')\n",
        "df.rename(columns={\n",
        "    'order_id': 'id',\n",
        "    'customer_name': 'customer',\n",
        "    'order_date': 'date',\n",
        "    'total_amount': 'amount'\n",
        "}, inplace=True)\n",
        "\n",
        "df.drop_duplicates(inplace=True)\n",
        "\n",
        "conn = sqlite3.connect('orders.db')\n",
        "cursor = conn.cursor()\n",
        "\n",
        "cursor.execute(\"\"\"\n",
        "CREATE TABLE IF NOT EXISTS orders (\n",
        "    id INTEGER PRIMARY KEY,\n",
        "    customer TEXT,\n",
        "    date TEXT,\n",
        "    amount REAL\n",
        ")\n",
        "\"\"\")\n",
        "\n",
        "df.to_sql('orders', conn, if_exists='replace', index=False)\n",
        "\n",
        "for row in cursor.execute(\"SELECT * FROM orders\"):\n",
        "    print(row)\n",
        "\n",
        "conn.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  2. Неструктурированные данные и «озёра» данных\n",
        "### Характеристики неструктурированных и полу-структурированных данных\n",
        "\n",
        "Неструктурированные данные — это данные, не имеющие заранее заданной модели или структуры, которую можно легко представить в виде таблицы. Примеры: текстовые документы (PDF, DOCX), изображения, аудио, видео, логи приложений, сообщения пользователей.\n",
        "\n",
        "Полу-структурированные данные — это данные, которые не хранятся в строгой реляционной модели, но содержат теги, ключи или другие маркеры для логической организации. Примеры: JSON, XML, YAML, CSV с вложенными значениями.\n",
        "\n",
        "### Концепция Data Lake\n",
        "\n",
        "Data Lake (озеро данных) — это централизованное хранилище, где можно сохранять данные в их \"сыром\" виде — структурированные, полу- и неструктурированные.\n",
        "\n",
        "#### Типовая структура Data Lake:\n",
        "\n",
        "* /raw          → сырые данные\n",
        "* /processed    → очищенные, нормализованные данные\n",
        "* /analytics    → агрегированные, готовые к отчётности\n",
        "\n",
        "На практике в роли \"озера\" часто выступают:\n",
        "    * Amazon S3;\n",
        "    * HDFS (Hadoop Distributed File System);\n",
        "    * Azure Data Lake Storage;\n",
        "    * Google Cloud Storage.\n",
        "\n",
        "#### MapReduce\n",
        "\n",
        "MapReduce — это программная модель обработки больших объёмов данных в распределённых системах, впервые представленная Google.\n",
        "\n",
        "Принцип:\n",
        "* Map: каждая запись обрабатывается функцией, которая генерирует пары ключ-значение.\n",
        "* Shuffle/Sort: промежуточные результаты сортируются и группируются.\n",
        "* Reduce: агрегируются значения с одинаковыми ключами."
      ],
      "metadata": {
        "id": "c6oO7VCkZ03Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas pyarrow\n"
      ],
      "metadata": {
        "id": "2foSYCekZIuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[\n",
        "  {\n",
        "    \"user\": {\n",
        "      \"id\": 1,\n",
        "      \"name\": \"Vova\"\n",
        "    },\n",
        "    \"message\": {\n",
        "      \"text\": \"Hello world!\",\n",
        "      \"likes\": 5\n",
        "    }\n",
        "  },\n",
        "  {\n",
        "    \"user\": {\n",
        "      \"id\": 2,\n",
        "      \"name\": \"Dima\"\n",
        "    },\n",
        "    \"message\": {\n",
        "      \"text\": \"Good to see you\",\n",
        "      \"likes\": 8\n",
        "    }\n",
        "  }\n",
        "]\n"
      ],
      "metadata": {
        "id": "ii0rRQSgZD-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "with open('data.json', 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "df = pd.json_normalize(data)\n",
        "\n",
        "df.rename(columns={\n",
        "    'user.id': 'user_id',\n",
        "    'user.name': 'user_name',\n",
        "    'post.text': 'message_text',\n",
        "    'post.likes': 'message_likes'\n",
        "}, inplace=True)\n",
        "\n",
        "df.to_parquet('flattened_parquet', index=False)\n",
        "\n",
        "print(\"Файл сохранён\")\n"
      ],
      "metadata": {
        "id": "O0S_seLUZBSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import boto3\n",
        "import json\n",
        "\n",
        "aws_access_key_id = 'access-key'\n",
        "aws_secret_access_key = 'secret-key'\n",
        "bucket_name = 'bucket'\n",
        "raw_key = 'data_lake/raw/messages.json'\n",
        "processed_key = 'data_lake/processed/clean_posts.parquet'\n",
        "\n",
        "s3_url = f's3://{bucket_name}/{raw_key}'\n",
        "df = pd.read_json(s3_url, storage_options={\n",
        "    \"key\": aws_access_key_id,\n",
        "    \"secret\": aws_secret_access_key\n",
        "})\n",
        "\n",
        "df = pd.json_normalize(df.to_dict(orient='records'))\n",
        "\n",
        "df.columns = df.columns.str.replace('.', '_')\n",
        "df.dropna(axis=1, how='all', inplace=True)\n",
        "\n",
        "processed_url = f's3://{bucket_name}/{processed_key}'\n",
        "df.to_parquet(processed_url, index=False, storage_options={\n",
        "    \"key\": aws_access_key_id,\n",
        "    \"secret\": aws_secret_access_key\n",
        "})\n",
        "\n",
        "print(f\"Данные сохранены в S3 по пути: {processed_key}\")\n"
      ],
      "metadata": {
        "id": "hSl8cY37ZUB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Потоковые данные\n",
        "### Что такое потоковые данные\n",
        "\n",
        "Потоковые данные (англ. streaming data) — это непрерывный, постоянно обновляющийся поток информации, поступающий в систему с минимальными задержками. Такие данные могут генерироваться различными источниками в реальном времени.\n",
        "\n",
        "В отличие от пакетной (batch) обработки, где данные поступают и обрабатываются блоками, потоковые данные обрабатываются \"на лету\", по мере их появления.\n",
        "\n",
        "Работа с потоками требует решения ряда технических задач:\n",
        "* Задержки (latency) - Необходимо обрабатывать данные с минимальной задержкой.\n",
        "Гарантии доставки -\tВозможны дубликаты, потери или нарушение порядка сообщений (нужно учитывать semantiku: at-least-once, exactly-once и т.д.).\n",
        "* Out-of-order события -\tСобытия могут приходить не по порядку, особенно в распределённых сетях.\n",
        "* Оконная агрегация -\tНужно агрегировать события по временны́м окнам (например, «в течение 5 минут»).\n",
        "* Масштабируемость - Потоки часто высокообъемные, требуется горизонтальное масштабирование.\n",
        "\n",
        "#### Лямбда-архитектура\n",
        "\n",
        "Модель, предложенная для объединения высокой скорости и точности данных. Состоит из трёх слоёв:\n",
        "\n",
        "    * Batch Layer — обрабатывает полные данные и формирует «истинное» представление (например, пересчёт всех показателей).\n",
        "\n",
        "    * Speed Layer — быстро реагирует на новые события, работает с потоками.\n",
        "\n",
        "    * Serving Layer — отдаёт объединённые результаты пользователю.\n",
        "\n",
        "#### Капа-архитектура\n",
        "\n",
        "Предлагает использовать только потоковую обработку:\n",
        "\n",
        "    * Все данные поступают в поток;\n",
        "\n",
        "    * Пересчёты возможны путём переигрывания потока (replay);\n",
        "\n",
        "    * Отказ от batch-слоя.\n",
        "\n",
        "\n",
        "### Использование Apache Pulsar в рамках анализа потоков\n",
        "\n",
        "В рамках практической части была реализована простая стриминговая система на основе Apache Pulsar, которая:\n",
        "\n",
        "    * имитирует поток данных от пользователей (producer отправляет события в топик events);\n",
        "\n",
        "    * обрабатывает события в реальном времени (consumer принимает сообщения и выводит их содержимое).\n",
        "\n",
        "В рамках архитектуры Капа, Pulsar может выступать как универсальный поток данных — как для реального времени, так и для отложенного анализа (через повторное проигрывание сообщений из топиков)."
      ],
      "metadata": {
        "id": "PH9Dw_Mma2pH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pulsar\n",
        "import json\n",
        "import time\n",
        "\n",
        "client = pulsar.Client('pulsar://localhost:6650')\n",
        "producer = client.create_producer('persistent://public/default/events')\n",
        "\n",
        "events = [\n",
        "    {\"user\": \"Vova\", \"data1\": \"Hello!\"},\n",
        "    {\"user\": \"Dima\", \"data2\": \"Hi!!\"},\n",
        "    {\"user\": \"Valya\", \"data3\": \"Good Day\"}\n",
        "]\n",
        "\n",
        "for event in events:\n",
        "    producer.send(json.dumps(event).encode('utf-8'))\n",
        "    print(f\"Отправлено сообщение: {event}\")\n",
        "    time.sleep(5)\n",
        "\n",
        "client.close()\n"
      ],
      "metadata": {
        "id": "YjBVRPEDaHHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pulsar\n",
        "import json\n",
        "\n",
        "client = pulsar.Client('pulsar://localhost:6650')\n",
        "consumer = client.subscribe('persistent://public/default/events', 'subscription')\n",
        "\n",
        "try:\n",
        "    while True:\n",
        "        msg = consumer.receive()\n",
        "        data = json.loads(msg.data())\n",
        "        print(f\"Получено сообщение: {data}\")\n",
        "        consumer.acknowledge(msg)\n",
        "except KeyboardInterrupt:\n",
        "    print(\"Остановка работы консьюмера\")\n",
        "finally:\n",
        "    client.close()\n"
      ],
      "metadata": {
        "id": "xufRimXpaJkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Федеративное обучение"
      ],
      "metadata": {
        "id": "1kFwGW85x2yQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Определение федеративного обучения\n",
        "Централизованный анализ предполагает, что все данные собираются в одном месте — в хранилище или аналитической платформе. Это даёт возможность комплексного анализа, но создаёт риски:\n",
        "\n",
        " *   нарушения конфиденциальности (например, при передаче медицинских данных);\n",
        " *   перегрузки каналов связи;\n",
        " *   роста задержек при передаче больших объёмов данных.\n",
        "\n",
        "Федеративное обучение (Federated Learning, FL) — это подход, при котором данные остаются на стороне клиентов, а обучение моделей происходит локально. Центр координирует только агрегацию параметров модели, но не получает сами данные.\n",
        "\n",
        "#### Преимущества:\n",
        "\n",
        " *   Защита приватности.\n",
        " *   Снижение трафика.\n",
        " *   Возможность обучения на чувствительных данных (банки, медицина, edge-устройства).\n",
        "\n",
        "#### Существуют различные архитектуры федеративного обучения:\n",
        "* Централизованное FL -\tОдин центральный сервер координирует обучение, агрегирует параметры моделей.\n",
        "* Децентрализованное - FL\tУзлы обмениваются параметрами напрямую, без сервера.\n",
        "* Гибридные системы -\tКомбинируют оба подхода\n",
        "\n",
        "#### Основные алгоритмы федеративного обучения\n",
        "\n",
        "* FedAvg - Базовый алгоритм: локальное обучение + усреднение весов.\n",
        "* FedSGD - Градиенты усредняются, а не веса.\n",
        "* FedProx -\tМодификация FedAvg с регуляризацией, лучше работает при неравномерных данных.\n",
        "* FedNova -\tКомпенсирует различия в количестве локальных итераций.\n",
        "* FedMA -\tИспользует согласование моделей, а не усреднение весов.\n"
      ],
      "metadata": {
        "id": "cQ8rGsxkb86e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Обучение модели с помощью Flower\n",
        "\n",
        "Была проведёна симуляция федеративного обучения на простом датасете MNIST с использованием библиотеки Flower.\n",
        "\n",
        "В программе используется базовый алгоритм FedAvg (Federated Averaging), который является стандартным алгоритмом федеративного обучения в Flower\n",
        "\n",
        "#### Работа алгоритма в программе\n",
        "\n",
        "1. Сервер инициализирует глобальную модель\n",
        "\n",
        "2. Каждый клиент:\n",
        "* Получает текущие глобальные веса (set_parameters)\n",
        "* Обучает модель на своих данных\n",
        "* Отправляет обновлённые веса серверу\n",
        "\n",
        "3. Сервер усредняет полученные веса\n"
      ],
      "metadata": {
        "id": "VbVvJiy8cWig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install flwr torch torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "uSxYnnRWcgJE",
        "outputId": "302dca25-8bc3-4e97-f8fd-1fcd3f5a1329"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flwr in /usr/local/lib/python3.11/dist-packages (1.18.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: cryptography<45.0.0,>=44.0.1 in /usr/local/lib/python3.11/dist-packages (from flwr) (44.0.1)\n",
            "Requirement already satisfied: grpcio!=1.65.0,<2.0.0,>=1.62.3 in /usr/local/lib/python3.11/dist-packages (from flwr) (1.72.1)\n",
            "Requirement already satisfied: iterators<0.0.3,>=0.0.2 in /usr/local/lib/python3.11/dist-packages (from flwr) (0.0.2)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from flwr) (2.0.2)\n",
            "Requirement already satisfied: pathspec<0.13.0,>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from flwr) (0.12.1)\n",
            "Requirement already satisfied: protobuf<5.0.0,>=4.21.6 in /usr/local/lib/python3.11/dist-packages (from flwr) (4.21.6)\n",
            "Requirement already satisfied: pycryptodome<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from flwr) (3.23.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=6.0.2 in /usr/local/lib/python3.11/dist-packages (from flwr) (6.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from flwr) (2.32.3)\n",
            "Requirement already satisfied: rich<14.0.0,>=13.5.0 in /usr/local/lib/python3.11/dist-packages (from flwr) (13.9.4)\n",
            "Requirement already satisfied: tomli<3.0.0,>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from flwr) (2.2.1)\n",
            "Requirement already satisfied: tomli-w<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from flwr) (1.2.0)\n",
            "Requirement already satisfied: typer<0.13.0,>=0.12.5 in /usr/local/lib/python3.11/dist-packages (from flwr) (0.12.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography<45.0.0,>=44.0.1->flwr) (1.17.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->flwr) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->flwr) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->flwr) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->flwr) (2025.4.26)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich<14.0.0,>=13.5.0->flwr) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich<14.0.0,>=13.5.0->flwr) (2.19.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<0.13.0,>=0.12.5->flwr) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<0.13.0,>=0.12.5->flwr) (1.5.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography<45.0.0,>=44.0.1->flwr) (2.22)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.5.0->flwr) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import flwr as fl\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch import nn, optim\n",
        "import numpy as np\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer = nn.Linear(784, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layer(x.view(-1, 784))\n",
        "\n",
        "\n",
        "def load_data(client_id, num_clients):\n",
        "    transform = transforms.ToTensor()\n",
        "    train_dataset = torchvision.datasets.MNIST(\".\", train=True, download=True, transform=transform)\n",
        "\n",
        "    targets = np.array(train_dataset.targets)\n",
        "    idx = np.where(targets % num_clients == client_id)[0]\n",
        "\n",
        "    subset = Subset(train_dataset, idx)\n",
        "    loader = torch.utils.data.DataLoader(subset, batch_size=32, shuffle=True)\n",
        "    return loader\n",
        "\n",
        "\n",
        "class FlowerClient(fl.client.NumPyClient):\n",
        "    def __init__(self, client_id, num_clients):\n",
        "        self.model = Net()\n",
        "        self.loader = load_data(client_id, num_clients)\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.optimizer = optim.SGD(self.model.parameters(), lr=0.01)\n",
        "\n",
        "    def get_parameters(self, config): return [val.cpu().numpy() for val in self.model.parameters()]\n",
        "    def set_parameters(self, params):\n",
        "        for p, val in zip(self.model.parameters(), params):\n",
        "            p.data = torch.tensor(val, dtype=p.dtype)\n",
        "\n",
        "    def fit(self, params, config):\n",
        "        self.set_parameters(params)\n",
        "        for data, target in self.loader:\n",
        "            self.optimizer.zero_grad()\n",
        "            output = self.model(data)\n",
        "            loss = self.criterion(output, target)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "        return self.get_parameters(config), len(self.loader.dataset), {}\n",
        "\n",
        "    def evaluate(self, params, config):\n",
        "        return 0.5, len(self.loader.dataset), {}"
      ],
      "metadata": {
        "id": "GtfVpnLrcTvO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    client_id = int(sys.argv[1])\n",
        "    num_clients = 3\n",
        "\n",
        "    client = FlowerClient(client_id, num_clients)\n",
        "    fl.client.start_numpy_client(server_address=\"localhost:8080\", client=client)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "collapsed": true,
        "id": "0ZMQd1mjl9O5",
        "outputId": "5ebcbe48-df4a-4656-dcb8-c659dba4d0a4"
      },
      "execution_count": 5,
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Запуск скрипта происходит через:\n",
        "`python script.py i `, где i - идентификатор клиента"
      ],
      "metadata": {
        "id": "nk3yDVw9zVHl"
      }
    }
  ]
}
